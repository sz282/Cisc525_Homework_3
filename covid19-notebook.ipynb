{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID19 Notebook\n",
    "website: `https://github.com/CSSEGISandData/COVID-19`\n",
    "\n",
    "```bash\n",
    "cd ~/cisc_525\n",
    "hdfs dfs -copyFromLocal COVID-19 /user/student\n",
    "\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "import subprocess\n",
    "from pyspark.sql.functions import col, max as max_\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"covid19-app\").config(\"spark.config.option\", \"value\").getOrCreate()\n",
    "scfg = SparkConf().setAppName('covid19-app')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 'hdfs://localhost:9000/user/student/csse_covid_19_data/csse_covid_19_daily_reports_us/05-15-2020.csv'\n",
    "time_series = 'hdfs://localhost:9000/user/student/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = spark.read.option('header', 'true').csv(time_series)\n",
    "day_df = spark.read.option('header', 'true').csv(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from a dataframe df to a resilient distributed data rdd\n",
    "ts_rdd = ts_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Column number\n",
    "\n",
    "# UID,iso2,iso3,code3,FIPS,Admin2,Province_State,Country_Region,Lat,Long_,Combined_Key,\n",
    "# 1/22/20,1/23/20,1/24/20,1/25/20,1/26/20,1/27/20,1/28/20,1/29/20,1/30/20,1/31/20,2/1/20,\n",
    "# 2/2/20,2/3/20,2/4/20,2/5/20,2/6/20,2/7/20,2/8/20,2/9/20,2/10/20,2/11/20,2/12/20,2/13/20,\n",
    "# 2/14/20,2/15/20,2/16/20,2/17/20,2/18/20,2/19/20,2/20/20,2/21/20,2/22/20,2/23/20,2/24/20,\n",
    "# 2/25/20,2/26/20,2/27/20,2/28/20,2/29/20,3/1/20,3/2/20,3/3/20,3/4/20,3/5/20,3/6/20,3/7/20,\n",
    "# 3/8/20,3/9/20,3/10/20,3/11/20,3/12/20,3/13/20,3/14/20,3/15/20,3/16/20,3/17/20,3/18/20,\n",
    "# 3/19/20,3/20/20,3/21/20,3/22/20,3/23/20,3/24/20,3/25/20,3/26/20,3/27/20,3/28/20,3/29/20,\n",
    "# 3/30/20,3/31/20,4/1/20,4/2/20,4/3/20,4/4/20,4/5/20,4/6/20,4/7/20,4/8/20,4/9/20,4/10/20,\n",
    "# 4/11/20,4/12/20,4/13/20,4/14/20,4/15/20,4/16/20,4/17/20,4/18/20,4/19/20,4/20/20,4/21/20,\n",
    "# 4/22/20,4/23/20,4/24/20,4/25/20,4/26/20,4/27/20,4/28/20,4/29/20,4/30/20,5/1/20,5/2/20,\n",
    "# 5/3/20,5/4/20,5/5/20,5/6/20,5/7/20,5/8/20,5/9/20,5/10/20,5/11/20,5/12/20,5/13/20,5/14/20,\n",
    "# 5/15/20\n",
    "\n",
    "TS_COLUMNS = ['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State',\n",
    "              'Country_Region', 'Lat', 'Long_', 'Combined_Key']\n",
    "\n",
    "TS_DATE_START_COLUMN = 11\n",
    "\n",
    "ts_first_row = ts_rdd.first()\n",
    "ts_first_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through the list of values of the first row\n",
    "for val in ts_first_row:\n",
    "    print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the list of values of first row using column name with ROW data type\n",
    "for key in TS_COLUMNS:\n",
    "    print(key, '=', ts_first_row[key])\n",
    "    \n",
    "# ts_by_dates = ts_first[TS_DATE_START_COLUMN:]\n",
    "# for ts_by_date in ts_by_dates:\n",
    "#     print(ts_by_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something about date and time\n",
    "start_date = datetime.date(2020, 1, 22)\n",
    "print (start_date.strftime('%m/%d/%y'))\n",
    "start_date += datetime.timedelta(days=1)\n",
    "cur_date_str = '{}/{}/{}'.format(start_date.month, start_date.day, start_date.year-2000)\n",
    "print (start_date.strftime('%0m/%d/%y'))\n",
    "print(cur_date_str)\n",
    "print(cur_date_str == '1/23/20')\n",
    "\n",
    "cur_date = datetime.date(2020, 1, 22)\n",
    "DATE_COLUMNS = []\n",
    "while True:\n",
    "    cur_date += datetime.timedelta(days=1)\n",
    "    cur_date_str = '{}/{}/{}'.format(cur_date.month, cur_date.day, cur_date.year-2000)\n",
    "    print(cur_date_str)\n",
    "    DATE_COLUMNS.append(cur_date_str)\n",
    "    if cur_date_str == '6/7/20':\n",
    "        break\n",
    "\n",
    "for date_str in DATE_COLUMNS:\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Printing out the content of the values by date columns. \n",
    "# Date columns are to extend over time.\n",
    "\n",
    "start_date = datetime.date(2020, 1, 22)\n",
    "ts_first = ts_rdd.first();\n",
    "ts_by_dates = ts_first[TS_DATE_START_COLUMN:]\n",
    "\n",
    "# for ts_by_date in ts_by_dates:\n",
    "#     print(ts_by_date)\n",
    "    \n",
    "for date_str in DATE_COLUMNS:\n",
    "    print(date_str, '=', ts_first[date_str])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Province or State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we group by provice or state, we get a list of nodes. each node\n",
    "# consists of the key (name of the state) and a list of da rows for each \n",
    "# of the states.\n",
    "\n",
    "ts_states = ts_rdd.groupBy(lambda x: x['Province_State'])\n",
    "# dir(ts_states)\n",
    "sorted_by_states = ts_states.sortByKey('Province_State')\n",
    "# print(sorted_by_states)\n",
    "\n",
    "sorted_by_states.collect()\n",
    "\n",
    "for row in sorted_by_states.collect():\n",
    "    print(row[0], len(row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(ts_states.collect()))\n",
    "for state in sorted(ts_states.collect()):\n",
    "    print(state[0], len(state[1]))\n",
    "    for item in state[1]:\n",
    "        print ('\\t', item['Admin2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_filtered = ts_rdd.filter(lambda x: x['Admin2'] != None)\n",
    "ts_admin2s = ts_rdd.groupBy(lambda x: x['Admin2'])\n",
    "# ts_states\n",
    "ts_admin2s = ts_admin2s.filter(lambda x: x[0] != None)\n",
    "# print(ts_admin2s.collect())\n",
    "print(len(ts_admin2s.collect()))\n",
    "for admin2 in sorted(ts_admin2s.collect()):\n",
    "    print(admin2[0], len(admin2[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.first()['UID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ts_df.groupBy('Province_State').count().orderBy('Province_State')\n",
    "out.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.first()['Province_State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = ts_df.first()\n",
    "print(first_row.UID)\n",
    "print(first_row['UID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.select('Province_State').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.sort('Province_State').select('Province_State').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts_df.filter(ts_df['5/15/20'] != '0').select('Province_State', '5/15/20').orderBy(desc('5/15/20')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = '6/7/20'\n",
    "target = ts_df.select('Admin2', 'Province_State', target_date).where(ts_df['Admin2'] != 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.Admin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, StringType, StructType, IntegerType)\n",
    "data_fields = [StructField('Admin2', StringType(), True), StructField('Province_State', StringType(), True),\n",
    "               StructField(target_date, StringType(), True)]\n",
    "data_schema = StructType(data_fields)\n",
    "newDF = spark.createDataFrame(target.rdd, schema=data_schema)\n",
    "newDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_list = []\n",
    "for row in target.collect():\n",
    "    target_list.append(['{}.{}'.format(row['Province_State'], row['Admin2']), int(row[target_date])])\n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(x):\n",
    "    return x[1]\n",
    "\n",
    "target_list\n",
    "sorted(target_list, key = get_key, reverse=True)\n",
    "\n",
    "# 'New York.New York', 206969 Jun 7\n",
    "# 'New York.New York', 206511 Jun 6\n",
    "# 'Georgia.Fulton', 4823 Jun 7\n",
    "# 'Georgia.Fulton', 4822 Jun 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End DF demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_by_country_region (day):\n",
    "    df = spark.read.option(\"header\", \"true\").csv(day)\n",
    "    return df.groupBy('Country_region').count().orderBy(desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_by_country_region(day).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "dir_in = \"/user/student/covid19/daily\"\n",
    "args = \"hdfs dfs -ls \"+dir_in+\" | awk '{print $8}'\"\n",
    "proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "s_output, s_err = proc.communicate()\n",
    "all_dart_dirs = s_output.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
